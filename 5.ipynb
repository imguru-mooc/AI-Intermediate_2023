{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드 그래프 와 미분 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = 1 / (1+np.exp(-x))\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y*(1-y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh 그래프 와 미분 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = np.tanh(x)\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,1-y**2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu 그래프 와 미분 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = np.maximum(0,x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우의 자동 미분 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = tf.Variable(np.array([1.0, 2.0, 3.0]))\n",
    "# x = np.array([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 3 + 2 * x + 5\n",
    "    \n",
    "print(y)\n",
    "# 그래디언트를 계산합니다.\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 신경망을 이용한 이미지 분류 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ConvolutionNetwork:\n",
    "    \n",
    "    def __init__(self, n_kernels=10, units=10, batch_size=32, learning_rate=0.1):\n",
    "        self.n_kernels = n_kernels  # 합성곱의 커널 개수\n",
    "        self.kernel_size = 3        # 커널 크기\n",
    "        self.optimizer = None       # 옵티마이저\n",
    "        self.conv_w = None          # 합성곱 층의 가중치\n",
    "        self.conv_b = None          # 합성곱 층의 절편\n",
    "        self.units = units          # 은닉층의 뉴런 개수\n",
    "        self.batch_size = batch_size  # 배치 크기\n",
    "        self.w1 = None              # 은닉층의 가중치\n",
    "        self.b1 = None              # 은닉층의 절편\n",
    "        self.w2 = None              # 출력층의 가중치\n",
    "        self.b2 = None              # 출력층의 절편\n",
    "        self.a1 = None              # 은닉층의 활성화 출력\n",
    "        self.losses = []            # 훈련 손실\n",
    "        self.val_losses = []        # 검증 손실\n",
    "        self.lr = learning_rate     # 학습률\n",
    "\n",
    "    def forpass(self, x):\n",
    "        # 3x3 합성곱 연산을 수행합니다.\n",
    "#         print('x.shape',x.shape)                     # (128,28,28,1)\n",
    "#         print('self.conv_w.shape',self.conv_w.shape) # (3,3,1,10)\n",
    "#         print('self.conv_w',self.conv_w)\n",
    "        c_out = tf.nn.conv2d(x, self.conv_w, strides=1, padding='SAME') + self.conv_b\n",
    "        # 렐루 활성화 함수를 적용합니다.\n",
    "#         print('c_out.shape',c_out.shape)             # (128,28,28,10)\n",
    "#         print('c_out',c_out)\n",
    "        r_out = tf.nn.relu(c_out)\n",
    "#         print('r_out.shape',r_out.shape)             # (128,28,28,10)\n",
    "#         print('r_out',r_out)\n",
    "        # 2x2 최대 풀링을 적용합니다.\n",
    "        p_out = tf.nn.max_pool2d(r_out, ksize=2, strides=2, padding='VALID')\n",
    "        # 첫 번째 배치 차원을 제외하고 출력을 일렬로 펼칩니다.\n",
    "#         print('p_out.shape',p_out.shape)             # (128,14,14,10)\n",
    "#         print('p_out',p_out)\n",
    "        f_out = tf.reshape(p_out, [x.shape[0], -1])    # (128,1960)\n",
    "#         print('f_out.shape',f_out.shape)\n",
    "        z1 = tf.matmul(f_out, self.w1) + self.b1     # 첫 번째 층의 선형 식을 계산합니다\n",
    "                                                     # (128,1960)(1960,100)+(100,)=(128,100)\n",
    "        a1 = tf.nn.relu(z1)                          # 활성화 함수를 적용합니다\n",
    "        z2 = tf.matmul(a1, self.w2) + self.b2        # 두 번째 층의 선형 식을 계산합니다.\n",
    "                                                     # (128,100)(100,10)+(10,)=(128,10)\n",
    "        return z2\n",
    "    \n",
    "    def init_weights(self, input_shape, n_classes):\n",
    "        g = tf.initializers.glorot_uniform()\n",
    "        self.conv_w = tf.Variable(g((3, 3, 1, self.n_kernels)))\n",
    "        self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype=float)\n",
    "        n_features = 14 * 14 * self.n_kernels\n",
    "        self.w1 = tf.Variable(g((n_features, self.units)))          # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = tf.Variable(np.zeros(self.units), dtype=float)    # 은닉층의 크기\n",
    "        self.w2 = tf.Variable(g((self.units, n_classes)))           # (은닉층의 크기, 클래스 개수)\n",
    "        self.b2 = tf.Variable(np.zeros(n_classes), dtype=float)     # 클래스 개수\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        self.init_weights(x.shape, y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        self.optimizer = tf.optimizers.SGD(learning_rate=self.lr)\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            print('에포크', i, end=' ')\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                print('.', end='')\n",
    "                self.training(x_batch, y_batch) # x_batch.shape  (128, 28, 28, 1)\n",
    "                # 배치 손실을 기록합니다.\n",
    "                batch_losses.append(self.get_loss(x_batch, y_batch))\n",
    " \n",
    "            print()\n",
    "            # 배치 손실 평균내어 훈련 손실 값으로 저장합니다.\n",
    "            self.losses.append(np.mean(batch_losses))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.val_losses.append(self.get_loss(x_val, y_val))\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        bins = len(x) // self.batch_size                   # 미니배치 횟수\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다.\n",
    "            \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                    # 샘플 개수를 저장합니다.\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "            # 손실을 계산합니다.\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(y, z)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        weights_list = [self.conv_w, self.conv_b,\n",
    "                        self.w1, self.b1, self.w2, self.b2]\n",
    "        # 가중치에 대한 그래디언트를 계산합니다.\n",
    "        grads = tape.gradient(loss, weights_list)\n",
    "        # 가중치를 업데이트합니다.\n",
    "        self.optimizer.apply_gradients(zip(grads, weights_list))\n",
    "   \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)                 # 정방향 계산을 수행합니다.\n",
    "        return np.argmax(z.numpy(), axis=1) # 가장 큰 값의 인덱스를 반환합니다.\n",
    "              #   5\n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis=1))\n",
    "                            # 5               5\n",
    "\n",
    "    def get_loss(self, x, y):\n",
    "        z = self.forpass(x)                 # 정방향 계산을 수행합니다.\n",
    "        # 손실을 계산하여 저장합니다.\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, z))\n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_all.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, \n",
    "                                                  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_val = x_val.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_val = x_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ConvolutionNetwork(n_kernels=10, units=100, batch_size=128, learning_rate=0.01)\n",
    "cn.fit(x_train, y_train_encoded, \n",
    "       x_val=x_val, y_val=y_val_encoded, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cn.losses)\n",
    "plt.plot(cn.val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn.score(x_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스로 합성곱 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "conv1 = tf.keras.Sequential()\n",
    "conv1.add(Conv2D(10, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "                                 #  conv_w.shape = (3,3,1,10) , conv_b = (10,)\n",
    "conv1.add(MaxPooling2D((2, 2)))\n",
    "conv1.add(Flatten())\n",
    "conv1.add(Dense(100, activation='relu'))   #  w1.shape = (1960,100), b1.shape = (100,)\n",
    "conv1.add(Dense(10, activation='softmax')) #  w2.shape = (100,10),   b2.shape = (10,)\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = conv1.fit(x_train, y_train_encoded, epochs=20, \n",
    "                    validation_data=(x_val, y_val_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭 아웃 층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "conv2 = tf.keras.Sequential()\n",
    "conv2.add(Conv2D(10, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "conv2.add(MaxPooling2D((2, 2)))\n",
    "conv2.add(Flatten())\n",
    "conv2.add(Dropout(0.5))\n",
    "conv2.add(Dense(100, activation='relu'))\n",
    "conv2.add(Dense(10, activation='softmax'))\n",
    "conv2.summary()\n",
    "conv2.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = conv2.fit(x_train, y_train_encoded, epochs=20, \n",
    "                    validation_data=(x_val, y_val_encoded))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
